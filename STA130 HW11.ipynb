{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95f8be08",
   "metadata": {},
   "source": [
    "1.\n",
    "\n",
    "(a) Understanding the Problem Addressed by a Classification Decision Tree\n",
    "A Classification Decision Tree is designed to tackle classification issues, where the objective is to categorize an input into one of several established classes. This differs from regression issues, which yield continuous output values. \n",
    "Real-World Examples:\n",
    "- Medical Diagnosis: Assessing whether a patient has a specific illness (e.g., \"Yes\" or \"No\") based on symptoms and test results.\n",
    "- Spam Detection: Classifying emails as \"Spam\" or \"Not Spam\" based on their content.\n",
    "- Customer Segmentation: Identifying customer categories (e.g., \"High-spending,\" \"Medium-spending,\" \"Low-spending\") based on buying patterns.\n",
    "- Credit Risk Assessment: Evaluating whether a borrower is \"Low Risk\" or \"High Risk\" based on their financial history.\n",
    "In these examples, the outputs are categorical, and a decision tree aids in classifying inputs into these categories.\n",
    "\n",
    "(b) Comparison of Predictions Between Classification Decision Trees and Multiple Linear Regression\n",
    "Classification Decision Trees:\n",
    "A Classification Decision Tree operates by dividing the dataset into smaller groups according to specific decision rules at each \"node.\" These rules are based on the values of input features, such as \"Is Age > 30?\" or \"Is Income <= $50,000?\" The tree continues to split until it reaches a \"leaf node,\" where a final classification is made. For example, in a binary classification scenario, the leaf may predict \"Class A\" if the majority of training samples at that leaf belong to \"Class A.\"\n",
    "Key Features:\n",
    "- Predictions are categorical (e.g., \"Spam\" vs. \"Not Spam\").\n",
    "- Splits are determined by optimizing metrics like information gain or Gini impurity, which help identify the best feature and threshold for a split.\n",
    "- It follows a sequential decision-making process, with each split refining the options.\n",
    "\n",
    "Multiple Linear Regression:\n",
    "Regression predicts a continuous outcome through a weighted linear combination of input features: \\( \\hat{y} = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\ldots + \\beta_nx_n \\). Each feature influences the final prediction according to its coefficient (\\( \\beta_i \\)). The model seeks to minimize the sum of squared errors (SSE) to find the best-fitting line.\n",
    "Key Features: \n",
    "- Predictions are numerical (e.g., price, weight, or probability).\n",
    "- The relationships between features and the target variable are linear.\n",
    "\n",
    "2.\n",
    "\n",
    "\n",
    "1. Accuracy\n",
    "Context: Manufacturing Quality Control\n",
    "\n",
    "Description: Accuracy is useful for assessing overall production quality when defects are infrequent. For instance, in a light bulb manufacturing facility, this metric can indicate the percentage of bulbs that function correctly (both those that pass and fail quality assessments).\n",
    "Justification: When the repercussions of misclassification (such as a defective bulb being marked as functional or the opposite) are minimal and the class distribution is even, accuracy offers a straightforward view of overall effectiveness.\n",
    "\n",
    "2. Sensitivity\n",
    "Context: Disease Screening in Healthcare (e.g., Cancer Detection)\n",
    "\n",
    "Description: Sensitivity is crucial for ensuring that most patients with a disease are accurately identified. For example, in breast cancer screenings, high sensitivity helps to reduce the number of false negatives (failing to identify actual patients).\n",
    "Justification: Failing to detect a positive case (FN) can have serious implications, making it essential to maximize the identification of true positives.\n",
    "\n",
    "3. Specificity\n",
    "Context: Filtering Spam Emails\n",
    "\n",
    "Description: Specificity is key for accurately identifying legitimate emails. For example, a spam filter should exhibit high specificity to prevent mistakenly categorizing genuine emails (true negatives) as spam (false positives).\n",
    "Justification: Incorrectly marking legitimate emails (FP) as spam can lead to the loss of important communications, so maintaining high specificity is vital for user confidence.\n",
    "\n",
    "4. Precision\n",
    "Context: Banking Fraud Detection\n",
    "\n",
    "Description: Precision is critical when detecting fraudulent transactions, as false positives (legitimate transactions flagged as fraudulent) can negatively impact customer experience.\n",
    "Justification: A model with high precision reduces the number of non-fraudulent transactions incorrectly identified as fraud, ensuring customer satisfaction while concentrating on actual fraud cases.\n",
    "\n",
    "3.\n",
    "\n",
    "Preprocessing Steps:\n",
    "1. Remove Specific Columns: Eliminate Weight_oz, Width, and Height. \n",
    "2. Address Missing Values: Delete any rows with NaN entries after filtering to the relevant columns.\n",
    "3. Modify Data Types:\n",
    "   - Change Pub year and NumPages to integer type.\n",
    "   - Convert Hard_or_Paper to a categorical data type.\n",
    "4. Create a Cleaned DataFrame: Label this version as ab_reduced_noNaN.\n",
    "\n",
    "Exploratory Data Analysis (EDA):\n",
    "1. Summary Statistics:\n",
    "   - Provide essential statistics for numerical columns (Pub year, NumPages) including mean, median, minimum, maximum, and standard deviation.\n",
    "2. Categorical Distribution:\n",
    "   - Count the occurrences of each category in Hard_or_Paper.\n",
    "3. Initial Insights:\n",
    "   - Identify trends such as publication patterns over the years or average page counts for various book types.\n",
    "\n",
    "4.\n",
    "\n",
    "\n",
    "Step 1: Data Splitting\n",
    "Conduct an 80/20 split of the ab_reduced_noNaN dataset into training and testing sets, ensuring to use a random seed for consistency. Provide the count of observations in both the training and testing datasets.\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Specify features and labels\n",
    "X = ab_reduced_noNaN[['List Price']]\n",
    "y = pd.get_dummies(ab_reduced_noNaN[\"Hard_or_Paper\"])['H']  # 1 indicates Hard, 0 indicates Paper\n",
    "# Execute the 80/20 split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# Output sizes\n",
    "print(f\"Number of observations in training set: {len(X_train)}\")\n",
    "print(f\"Number of observations in test set: {len(X_test)}\")\n",
    "ChatBot Question: DecisionTreeClassifier\n",
    "Inquire with ChatBot:\n",
    "\"What is the purpose of the .fit() method in a scikit-learn DecisionTreeClassifier? And what does .predict() do?\"\n",
    "Anticipated ChatBot Answer:\n",
    ".fit(X, y):\n",
    "This method trains the decision tree model using the specified training data X (features) and y (labels).\n",
    "It learns the decision-making rules by optimizing a criterion (like Gini Impurity or Information Gain) to classify the target variable.\n",
    ".predict(X):\n",
    "Once trained, the model applies the learned rules to forecast labels for new data points in X.\n",
    "Step 2: Train the Decision Tree\n",
    "Utilize a DecisionTreeClassifier to train on the List Price variable to determine if a book is hardcover or paperback, setting max_depth to 2 for simplicity.\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "# Create the DecisionTreeClassifier instance\n",
    "clf = DecisionTreeClassifier(max_depth=2, random_state=42)\n",
    "# Train the classifier with the training data\n",
    "clf.fit(X_train, y_train)\n",
    "# Visualize the trained tree\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10, 6))\n",
    "tree.plot_tree(clf, feature_names=['List Price'], class_names=['Paper', 'Hard'], filled=True)\n",
    "plt.show()\n",
    "Understanding the Predictions\n",
    "Tree Structure:\n",
    "The root node divides based on the List Price. For instance:\n",
    "If List Price <= $20, the model may predict \"Paperback.\"\n",
    "If List Price > $20, it may predict \"Hardcover\" or make additional splits.\n",
    "Node and Leaf Interpretation:\n",
    "Each node signifies a decision rule based on List Price.\n",
    "Each leaf node indicates the predicted class (hardcover or paperback) and the sample distribution that leads to that node.\n",
    "Reason for Training on Training Data\n",
    "The decision tree should be trained solely on the training data (ab_reduced_noNaN_train) to prevent information leakage. Training on the test set would compromise the model's ability to generalize to new data, as the test set is meant for evaluation only.\n",
    "\n",
    "Summary: ### Summary of Your Requests and Steps to Address Them\n",
    "\n",
    "1. Exploratory Data Analysis (EDA)\n",
    "You asked to preprocess the amazonbooks.csv dataset and perform initial exploratory data analysis.\n",
    "\n",
    "Steps Taken:\n",
    "Preprocessing:\n",
    "\n",
    "Removed columns: Weight_oz, Width, and Height.\n",
    "Dropped rows with missing (NaN) values.\n",
    "Converted data types:\n",
    "Pub year and NumPages → int.\n",
    "Hard_or_Paper → category.\n",
    "EDA Tasks:\n",
    "\n",
    "Displayed summary statistics for numerical columns (Pub year, NumPages).\n",
    "Counted frequencies of categories in Hard_or_Paper.\n",
    "2. 80/20 Train-Test Split\n",
    "You wanted to split the data into training and test sets for reproducible machine learning.\n",
    "\n",
    "Steps Taken:\n",
    "Used train_test_split from scikit-learn to create:\n",
    "80% training set: X_train, y_train.\n",
    "20% testing set: X_test, y_test.\n",
    "Set a random seed for reproducibility (random_state=42).\n",
    "Reported the number of observations in the training and test datasets.\n",
    "3. Decision Tree Classification\n",
    "You wanted to train a DecisionTreeClassifier to predict whether a book is a hardcover or paperback based on the List Price feature.\n",
    "\n",
    "Steps Taken:\n",
    "ChatBot Inquiry:\n",
    "\n",
    "Asked what .fit() and .predict() methods do:\n",
    ".fit() trains the tree on data (X_train, y_train).\n",
    ".predict() uses the trained model to predict labels on new data.\n",
    "Model Fitting:\n",
    "\n",
    "Trained a DecisionTreeClassifier using List Price as the feature and a max_depth of 2.\n",
    "Used ab_reduced_noNaN_train for training, ensuring no information leakage.\n",
    "Visualization:\n",
    "\n",
    "Visualized the tree with tree.plot_tree(clf), showing decision rules and predictions.\n",
    "Explained how splits (e.g., thresholds on List Price) guide classification.\n",
    "4. Why Fit on Training Data Only?\n",
    "Fitting the model on training data ensures the test data remains unseen, preserving its integrity for unbiased evaluation. Using the test set during training would lead to overfitting and invalid generalization performance.\n",
    "\n",
    "Link: https://chatgpt.com/share/673ef326-1420-800e-9f55-b82ee3a804b9\n",
    "\n",
    "5.\n",
    "\n",
    "Establish the New Feature Set:\n",
    "In this scenario, you are utilizing several features (NumPages, Thick, and List Price) as inputs for the decision tree classifier, while the target variable (y) remains unchanged.\n",
    "Train the Decision Tree Classifier:\n",
    "We will train a new decision tree (clf2) using the revised feature set, with a maximum depth set to 4.\n",
    "\n",
    "Code:\n",
    "# Define the new feature set\n",
    "X_multi = ab_reduced_noNaN[['NumPages', 'Thick', 'List Price']]\n",
    "y = pd.get_dummies(ab_reduced_noNaN[\"Hard_or_Paper\"])['H']\n",
    "# Perform an 80/20 split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_multi, y, test_size=0.2, random_state=42)\n",
    "# Initialize and train the classifier\n",
    "clf2 = DecisionTreeClassifier(max_depth=4, random_state=42)\n",
    "clf2.fit(X_train, y_train)\n",
    "# Visualize the decision tree\n",
    "plt.figure(figsize=(15, 10))\n",
    "tree.plot_tree(\n",
    "    clf2,\n",
    "    feature_names=['NumPages', 'Thick', 'List Price'],\n",
    "    class_names=['Paper', 'Hard'],\n",
    "    filled=True\n",
    ")\n",
    "plt.show()\n",
    "3. Describe the Predictions Made by clf2\n",
    "General Prediction Process:\n",
    "Feature Evaluations:\n",
    "The decision tree begins at the root node, assessing a condition based on one of the features (NumPages, Thick, or List Price).\n",
    "Depending on the truth value of the condition, the model proceeds to either the left or right child node.\n",
    "Branching Decisions:\n",
    "The tree checks further conditions (e.g., Thick > 2 or NumPages <= 300) at each subsequent node.\n",
    "At each decision juncture, the data is split into smaller subsets according to feature thresholds.\n",
    "Leaf Nodes:\n",
    "Upon reaching a leaf node, the tree predicts a class (Hard or Paper) based on the majority class of the training samples present in that node.\n",
    "Visualization Example from tree.plot_tree:\n",
    "Root Node Example:\n",
    "If List Price <= 20, proceed to the left. If not, go to the right.\n",
    "Left Subtree Example:\n",
    "If NumPages <= 250, predict Paper.\n",
    "If NumPages > 250, predict Hard.\n",
    "Right Subtree Example:\n",
    "If Thick > 3, predict Hard.\n",
    "If Thick <= 3, predict Paper.\n",
    "Benefits of Utilizing Multiple Features\n",
    "Adds more dimensions to enhance model accuracy.\n",
    "Better manages complex decision boundaries compared to using a single feature.\n",
    "Captures interactions between features such as Thick and NumPages. Would you like help in assessing this model's performance on the test set or examining feature importance?\n",
    "\n",
    "6.\n",
    "\n",
    "We will use the test data (ab_reduced_noNaN_test) to evaluate both models (clf and clf2) by generating confusion matrices and calculating the metrics: sensitivity, specificity, and accuracy.\n",
    "\n",
    "1. Definitions\n",
    "Metrics:\n",
    "Accuracy:\n",
    "[ \\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN} ]\n",
    "Sensitivity (Recall for Positive Class):\n",
    "[ \\text{Sensitivity} = \\frac{TP}{TP + FN} ]\n",
    "Specificity (Recall for Negative Class):\n",
    "[ \\text{Specificity} = \\frac{TN}{TN + FP} ]\n",
    "2. Evaluate clf (Single Feature Model)\n",
    "Code:\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, recall_score\n",
    "\n",
    "# Predictions for clf\n",
    "y_pred_clf = clf.predict(X_test[['List Price']])\n",
    "\n",
    "# Confusion Matrix for clf\n",
    "cm_clf = confusion_matrix(y_test, y_pred_clf)\n",
    "tn_clf, fp_clf, fn_clf, tp_clf = cm_clf.ravel()\n",
    "\n",
    "# Metrics for clf\n",
    "accuracy_clf = accuracy_score(y_test, y_pred_clf)\n",
    "sensitivity_clf = recall_score(y_test, y_pred_clf)  # TP / (TP + FN)\n",
    "specificity_clf = tn_clf / (tn_clf + fp_clf)        # TN / (TN + FP)\n",
    "\n",
    "print(\"Metrics for clf (List Price Only):\")\n",
    "print(f\"Accuracy: {accuracy_clf:.3f}\")\n",
    "print(f\"Sensitivity: {sensitivity_clf:.3f}\")\n",
    "print(f\"Specificity: {specificity_clf:.3f}\")\n",
    "3. Evaluate clf2 (Multiple Feature Model)\n",
    "Code:\n",
    "# Predictions for clf2\n",
    "y_pred_clf2 = clf2.predict(X_test[['NumPages', 'Thick', 'List Price']])\n",
    "\n",
    "# Confusion Matrix for clf2\n",
    "cm_clf2 = confusion_matrix(y_test, y_pred_clf2)\n",
    "tn_clf2, fp_clf2, fn_clf2, tp_clf2 = cm_clf2.ravel()\n",
    "\n",
    "# Metrics for clf2\n",
    "accuracy_clf2 = accuracy_score(y_test, y_pred_clf2)\n",
    "sensitivity_clf2 = recall_score(y_test, y_pred_clf2)  # TP / (TP + FN)\n",
    "specificity_clf2 = tn_clf2 / (tn_clf2 + fp_clf2)       # TN / (TN + FP)\n",
    "\n",
    "print(\"\\nMetrics for clf2 (Multiple Features):\")\n",
    "print(f\"Accuracy: {accuracy_clf2:.3f}\")\n",
    "print(f\"Sensitivity: {sensitivity_clf2:.3f}\")\n",
    "print(f\"Specificity: {specificity_clf2:.3f}\")\n",
    "4. Results Interpretation\n",
    "You will get the following metrics for each model:\n",
    "\n",
    "Accuracy: Measures overall correctness.\n",
    "Sensitivity: Measures how well the model identifies the \"hardcover\" books.\n",
    "Specificity: Measures how well the model identifies the \"paperback\" books.\n",
    "Comparison:\n",
    "Expect clf2 (using multiple features) to generally outperform clf (using List Price alone) in accuracy, sensitivity, and specificity since it leverages additional information.\n",
    "\n",
    "7.\n",
    "\n",
    "The variations between the two confusion matrices stem from the fact that the first model (clf) relies solely on the List Price feature, whereas the second model (clf2) includes additional features such as NumPages, Thick, and List Price. By utilizing more features, clf2 is able to identify more intricate relationships between the predictors and the target variable, resulting in better predictions. As a result, the confusion matrix for clf2 is expected to display higher counts of true positives and true negatives compared to clf. Furthermore, the confusion matrices for clf and clf2 assessed on the test set are considered more trustworthy than those evaluated on the training set, as they assess the models' ability to generalize to new data. Confusion matrices from the training set often yield overly positive results because the models have already encountered this data during training, which can lead to overfitting.\n",
    "\n",
    "8.\n",
    "\n",
    "To assess the importance of features in the classification decision tree clf2 using scikit-learn, we can utilize the .feature_importances_ attribute. This attribute yields scores for each feature, reflecting their contribution to the splits in the tree. By combining this with the .feature_names_in_ attribute, we can associate the importance scores with their corresponding feature names.\n",
    "\n",
    "Inquire a ChatBot about Visualizing Feature Importances\n",
    "Question for ChatBot:\n",
    "\"What methods can I use to visualize feature importances for a DecisionTreeClassifier in scikit-learn, particularly for a model like clf2?\"\n",
    "\n",
    "Expected Answer:\n",
    "The ChatBot would clarify that you can access the importance scores through clf2.feature_importances_ and link them to feature names using clf2.feature_names_in_. For visualization, libraries such as matplotlib or seaborn can be employed to create bar plots.\n",
    "\n",
    "Code to Identify and Visualize Feature Importances\n",
    "import matplotlib.pyplot as plt\n",
    "# Obtain feature importances and feature names\n",
    "feature_importances = clf2.feature_importances_\n",
    "feature_names = clf2.feature_names_in_\n",
    "# Create a DataFrame for enhanced visualization\n",
    "importances_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': feature_importances\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "# Display the most important feature\n",
    "most_important_feature = importances_df.iloc[0]\n",
    "print(\"Most significant predictor:\")\n",
    "print(most_important_feature)\n",
    "# Visualize feature importances\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(importances_df['Feature'], importances_df['Importance'], color='skyblue')\n",
    "plt.title(\"Feature Importances for clf2\")\n",
    "plt.xlabel(\"Feature\")\n",
    "plt.ylabel(\"Importance\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "Results and Insights\n",
    "Most Significant Predictor:\n",
    "The feature with the highest importance score in clf2 will be listed first in the sorted DataFrame, indicating it is the most critical for making predictions.\n",
    "Visualization:\n",
    "The bar plot provides a ranked view of all predictors, illustrating their relative importance in an easily understandable format. Would you like a more detailed explanation of how feature importance is determined or help with interpreting the plot?\n",
    "\n",
    "9.\n",
    "\n",
    "In linear regression, the coefficients indicate how each predictor relates to the target variable, assuming that the other predictors are held constant. A positive or negative coefficient shows the direction and size of the effect, making it easy to interpret and additive. In contrast, decision trees assess feature importance by evaluating how much each feature contributes to the splits in the tree, which shows their role in decreasing impurity (like Gini index or entropy). Unlike linear regression, decision trees can account for non-linear relationships and interactions, meaning that feature importance values do not necessarily reflect direct, independent effects.\n",
    "\n",
    "10.\n",
    "\n",
    "I have reviewed the course-wiki textbook and interacted with chatbot to help me understand my tutorial. \n",
    "\n",
    "Summary:\n",
    "    \n",
    "Here's a summary of your recent questions and requests:\n",
    "\n",
    "1. **Exploring Classification Decision Trees:**\n",
    "   - Asked for a description of the type of problem a Classification Decision Tree addresses and examples of real-world applications.\n",
    "   - Wanted to understand the differences between how a Classification Decision Tree makes predictions versus how Multiple Linear Regression makes predictions.\n",
    "\n",
    "2. **Data Preprocessing for Amazon Books Dataset:**\n",
    "   - Requested to remove certain columns (`Weight_oz`, `Width`, and `Height`), set `Pub year` and `NumPages` to `int` type, and `Hard_or_Paper` to `category` type.\n",
    "\n",
    "3. **Data Splitting for Model Training:**\n",
    "   - Asked to split the `ab_reduced_noNaN` dataset into an 80/20 train-test split and report the number of observations in each set.\n",
    "   - Wanted to fit a `DecisionTreeClassifier` (`clf`) using the `List Price` feature and visualize the model’s predictions using `tree.plot_tree(clf)`.\n",
    "\n",
    "4. **Training and Visualizing Decision Tree with Multiple Features:**\n",
    "   - Requested to train `clf2` using multiple features (`NumPages`, `Thick`, `List Price`) and visualize the classification decision tree with `max_depth` set to 4.\n",
    "   - Asked for an explanation of how predictions are made for the `clf2` model.\n",
    "\n",
    "5. **Evaluation Using Confusion Matrices:**\n",
    "   - Asked to create confusion matrices for `clf` and `clf2` using the test set (`ab_reduced_noNaN_test`) and report on the sensitivity, specificity, and accuracy for each model.\n",
    "\n",
    "6. **Feature Importance Visualization:**\n",
    "   - Inquired about visualizing feature importances for `clf2` using `.feature_importances_` and `.feature_names_in_` to identify the most significant predictor variable.\n",
    "\n",
    "7. **Comparing Feature Importances in Trees vs Coefficients in Linear Models:**\n",
    "   - Requested a comparison between interpreting coefficients in linear regression models and feature importances in decision trees.\n",
    "\n",
    "---\n",
    "\n",
    "This covers all the topics you’ve inquired about recently, focusing on understanding and applying classification decision trees, evaluating model performance, and visualizing feature importances. If you need more details or further assistance on any of these topics, feel free to ask!\n",
    "\n",
    "Link: https://chatgpt.com/share/673ef326-1420-800e-9f55-b82ee3a804b9\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ab32aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
